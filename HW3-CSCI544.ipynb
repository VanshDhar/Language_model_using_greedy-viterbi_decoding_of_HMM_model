{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprting libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Vocabulary Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first task is to create a vocabulary using the training data. In HMM, one important problem when creating the vocabulary is to handle unknown words. One simple solution is to replace rare words whose occurrences are less than a threshold (e.g. 3) with a special token '< unk >'.\n",
    "## Task. Creating a vocabulary using the training data in the file train and output the vocabulary into a txt file named vocab.txt. The format of the vocabulary  file is that each line contains a word type, its index in the vocabulary and its occurrences, separated by the tab symbol '\\t'. The first line should be the special token '< unk >' and the following lines should be sorted by its occurrences in descending order. Note that we can only use the training data to create the vocabulary, without touching the development and test data. What is the selected threshold for unknown words replacement? What is the total size of your vocabulary and what is the total occurrences of the special token '< unk >' after replacement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below we first read the train file then compute the occurence of each unique word in the file and store it under a new column named 'occ', then we replace each word whose frequency is less than 2 with a special token '\\<unk>'. We also display the selected threshold for unkown word replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Selected Threshold for unkown word replacement is: 2\n"
     ]
    }
   ],
   "source": [
    "#reading the training file\n",
    "df = pd.read_csv(\"./data/train\", sep = \"\\t\", names = ['id', 'words', 'pos'])\n",
    "#finding the number of occurences for each word\n",
    "df['occ'] = df.groupby('words')[\"words\"].transform('size')\n",
    "#function to replace each word with <unk> whose occurrence is less than 2\n",
    "def replace(row):\n",
    "    if row.occ < 2:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return row.words\n",
    "#applying the replace function on train data\n",
    "df['words'] = df.apply(lambda row : replace(row), axis = 1)\n",
    "print('The Selected Threshold for unkown word replacement is: 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In cell below we store all the unique words in the train data along with their occurences in df_vocab (in descending order based on their occurences ) and then put the special token (\\<unk>) data row above all others. Then we add index to df_vocab and rearrange it according to requirement and eventually store the vocabulary data in 'vocab.txt'.  We also display the total size of our vocabulary and the total occurences of the special token '\\<unk>'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total size of our vocabulary is: 23183\n",
      "The total occurences of the special token '<unk>': 20011\n"
     ]
    }
   ],
   "source": [
    "#storing unique words in df_vocab and arranging them in descending order of their occurrence value\n",
    "df_vocab = df.words.value_counts().rename_axis('words').reset_index(name = 'occ')\n",
    "#finding the data row containing word <unk>\n",
    "df_unk = df_vocab[df_vocab['words'] == \"<unk>\"]\n",
    "#removing the data row with word as <unk> and moving it to the top\n",
    "index = df_vocab[df_vocab.words == \"<unk>\"].index\n",
    "df_vocab = df_vocab.drop(index)\n",
    "df_vocab = pd.concat([df_unk, df_vocab]).reset_index(drop = True)\n",
    "#storing the index of each vocabulary in df_vocab\n",
    "df_vocab['id'] = df_vocab.index + 1\n",
    "#rearranging the vocabulary dataset into the required format\n",
    "cols = df_vocab.columns.tolist()\n",
    "cols = [cols[0], cols[-1], cols[1]]\n",
    "df_vocab = df_vocab[cols]\n",
    "#storing the vocabulary dataset to 'vocab.txt'\n",
    "df_vocab.to_csv(\"vocab.txt\", sep=\"\\t\", header=None)\n",
    "print('The total size of our vocabulary is: {}'.format(df_vocab.shape[0]))\n",
    "print('The total occurences of the special token \\'<unk>\\': {}'.format(int(df_vocab[df_vocab[\"words\"] == \"<unk>\"].occ)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the cell below we store all the unique pos tags of our training data in 'tags' variable and store our training data in a list ( called sentences) consisting of lists(called sentence), where each sublist consists of tuples, where each tuple corresponds to one data row in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing all the unique tags in training data into 'tags' variable\n",
    "df_pos = df.pos.value_counts().rename_axis('pos').reset_index(name = 'count')\n",
    "pos_dict = dict(df_pos.values)\n",
    "tags = df_pos.pos.tolist()\n",
    "\n",
    "#storing the training data in a list( called sentences) consisting of lists(called sentence), \n",
    "#where each sublist consists of tuples, where each tuple corresponds to one data row in training data\n",
    "sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for row in df.itertuples():\n",
    "    if(row.id == 1 and first == 0):\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append((row.words, row.pos))\n",
    "sentences.append(sentence)\n",
    "#del(df_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second task is to learn an HMM from the training data. Remember that the solution of the emission and transition parameters in HMM are in the following formulation:\n",
    "## t(s/s') = count(s->s') / count(s)\n",
    "## e(x/s) = count(s->x)/ count(s)\n",
    "## where t( .|. ) is the transition parameter and e( .|. ) is the emission parameter.\n",
    "## Task. Learning a model using the training data in the file train and output the learned model into a model file in json format, named hmm.json. The model file should contain two dictionaries for the emission and transition parameters, respectively. The first dictionary, named transition, contains items with pairs of (s, s') as key and t(s/s') as value. The second dictionary, named emission, contains items with pairs of (s, x) as key and e(x/s) as value. How many transition and emission parameters in your HMM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below the get_trans_matrix function computes the transition matrix ,using the formula provided, on the train data. Similarly, the get_emission_matrix function computes the emission matrix ,using the formula provided, on the train data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_trans_matrix function computes the transition matrix for the given sentences\n",
    "def get_trans_matrix(sentences, tags):\n",
    "    tr_matrix = np.zeros((len(tags),len(tags)))\n",
    "\n",
    "    tag_occ = {}\n",
    "    for tag in range(len(tags)):\n",
    "        tag_occ[tag] = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            tag_occ[tags.index(sentence[i][1])] += 1\n",
    "            if i == 0: continue\n",
    "            tr_matrix[tags.index(sentence[i - 1][1])][tags.index(sentence[i][1])] += 1\n",
    "    \n",
    "    for i in range(tr_matrix.shape[0]):\n",
    "        for j in range(tr_matrix.shape[1]):\n",
    "            if(tr_matrix[i][j] == 0) : tr_matrix[i][j] = 1e-10\n",
    "            else: tr_matrix[i][j] /= tag_occ[i]\n",
    "\n",
    "    return tr_matrix\n",
    "\n",
    "#get_emission_matrix function computes the emission matrix for the given sentences\n",
    "def get_emission_matrix(tags, vocab, sentences):\n",
    "    em_matrix = np.zeros((len(tags), len(vocab)))\n",
    "\n",
    "    tag_occ = {}\n",
    "    for tag in range(len(tags)):\n",
    "        tag_occ[tag] = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, pos in sentence:\n",
    "            tag_occ[tags.index(pos)] +=1\n",
    "            em_matrix[tags.index(pos)][vocab.index(word)] += 1\n",
    "\n",
    "    for i in range(em_matrix.shape[0]):\n",
    "        for j in range(em_matrix.shape[1]):\n",
    "            if(em_matrix[i][j] == 0) : em_matrix[i][j] = 1e-10\n",
    "            else: em_matrix[i][j] /= tag_occ[i]\n",
    "\n",
    "    return em_matrix\n",
    "\n",
    "vocab = df_vocab.words.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below the get_trans_probs function coverts the transition matrix to transition dictionary and the get_emission_probs function coverts the emission matrix to emission dictionary, while the get_inital_prob function calculates the initial transition probability for each tag and the get_all_prob function generates the transition matrix,emission matrix, transition dictionary and the emission dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_trans_probs function coverts the transition matrix to transition dictionary\n",
    "def get_trans_probs(tags, tr_matrix,prior_prob):\n",
    "    tags_dict = {}\n",
    "\n",
    "    for i, tags in enumerate(tags):\n",
    "        tags_dict[i] = tags\n",
    "\n",
    "    trans_prob = {}\n",
    "    for i in range(tr_matrix.shape[0]):\n",
    "        trans_prob['(' + '<\\S>' + ',' + tags_dict[i] + ')'] = prior_prob[tags_dict[i]]\n",
    "    for i in range(tr_matrix.shape[0]):\n",
    "        for j in range(tr_matrix.shape[1]):\n",
    "            trans_prob['(' + tags_dict[i] + ',' + tags_dict[j] + ')'] = tr_matrix[i][j]\n",
    "\n",
    "\n",
    "    return trans_prob\n",
    "\n",
    "#get_emission_probs function coverts the emission matrix to emission dictionary\n",
    "def get_emission_probs(tags, vocab, em_matrix):\n",
    "    tags_dict = {}\n",
    "\n",
    "    for i, tags in enumerate(tags):\n",
    "        tags_dict[i] = tags\n",
    "\n",
    "    emission_probs = {}\n",
    "\n",
    "    for i in range(em_matrix.shape[0]):\n",
    "        for j in range(em_matrix.shape[1]):\n",
    "            emission_probs['(' + tags_dict[i] + ', ' + vocab[j] + ')'] = em_matrix[i][j]\n",
    "\n",
    "    return emission_probs\n",
    "\n",
    "#get_all_prob function generates the transition matrix,emission matrix, \n",
    "#transition dictionary and the emission dictionary\n",
    "def get_all_prob(tags, vocab, sentences,prior_prob):\n",
    "    tr_matrix = get_trans_matrix(sentences, tags)\n",
    "    em_matrix = get_emission_matrix(tags, vocab, sentences)\n",
    "                \n",
    "    transition_probability = get_trans_probs(tags, tr_matrix,prior_prob)\n",
    "    emission_probability = get_emission_probs(tags, vocab, em_matrix)\n",
    "\n",
    "    return transition_probability, emission_probability\n",
    "\n",
    "#get_inital_prob function calculates the initial transition probability for each tag\n",
    "def get_inital_prob(df, tags):\n",
    "    tags_start_occ = {}\n",
    "    total_start_sum = 0\n",
    "    for tag in tags:\n",
    "        tags_start_occ[tag] = 0\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        if(row[1] == 1):\n",
    "            tags_start_occ[row[3]]+=1\n",
    "            total_start_sum += 1\n",
    "    \n",
    "    prior_prob = {}\n",
    "    for key in tags_start_occ:\n",
    "        prior_prob[key] = tags_start_occ[key] / total_start_sum\n",
    "    \n",
    "    return prior_prob\n",
    "\n",
    "prior_prob = get_inital_prob(df, tags)\n",
    "trans_prob, em_prob = get_all_prob(tags, vocab, sentences,prior_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below we display the Transition and Emission parameters of our HMM model and store the Transition and Emission dictionaries in a json file named 'hmm.json'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Transition Parameters are: 2070\n",
      "The number of Emission Parameters are: 1043235\n"
     ]
    }
   ],
   "source": [
    "print('The number of Transition Parameters are: {}'.format(len(trans_prob)))\n",
    "print('The number of Emission Parameters are: {}'.format(len(em_prob)))\n",
    "#we store the transition and emission dictionaries in a json file named 'hmm.json'\n",
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump({\"transition\": trans_prob, \"emission\": em_prob}, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3: Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third task is to implement the greedy decoding algorithm with HMM.\n",
    "## Task. Implementing the greedy decoding algorithm and evaluate it on the development data. What is the accuracy on the dev data? Predicting the part-of-speech tags of the sentences in the test data and output the predictions in a file named greedy.out, in the same format of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below we read the validation file, store the validation data in a list( called sentences) consisting of lists(called sentence), where each sublist consists of tuples, where each tuple corresponds to one data row in validation data. We also write a greedy_decoding function that computes the state sequence for our HMM Model using the greedy decoding technique. Similarly, we define a measure_acc function that computes the accuracy of our model by comparing the the predicted tag sequence with groundtruth tag sequence. We then use the greedy_decoding function to get the predicted tags for our validation data and then compute and display the accuracy using the measure_acc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our Greedy Decoding HMM model on Development data: 0.9351132293121244\n"
     ]
    }
   ],
   "source": [
    "#reading the validation file\n",
    "validation_data = pd.read_csv(\"./data/dev\", sep = '\\t', names = ['id', 'words', 'pos'])\n",
    "validation_data['occ'] = validation_data.groupby('words')['words'].transform('size')\n",
    "\n",
    "#storing the validation data in a list( called sentences) consisting of lists(called sentence), \n",
    "#where each sublist consists of tuples, where each tuple corresponds to one data row in validation data\n",
    "valid_sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for row in validation_data.itertuples():\n",
    "    if(row.id == 1 and first == 0):\n",
    "        valid_sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append((row.words, row.pos))\n",
    "valid_sentences.append(sentence)\n",
    "\n",
    "#greedy_decoding function computes the state sequence for our HMM Model using the greedy decoding technique\n",
    "def greedy_decoding(trans_prob, em_prob, prior_prob, valid_sentences, tags):\n",
    "    sequences = []\n",
    "    total_score = []\n",
    "    for sen in valid_sentences:\n",
    "        prev_tag = None\n",
    "        seq = []\n",
    "        score = []\n",
    "        for i in range(len(sen)):\n",
    "            best_score = -1\n",
    "            for j in range(len(tags)):\n",
    "                state_score = 1\n",
    "                if i == 0:\n",
    "                    state_score *= prior_prob[tags[j]]\n",
    "                else:\n",
    "                    if str(\"(\" + prev_tag  + \",\" + tags[j] + \")\") in trans_prob:\n",
    "                        state_score *= trans_prob[\"(\" + prev_tag  + \",\" + tags[j] + \")\"]\n",
    "                \n",
    "                if str(\"(\" + tags[j] + \", \" + sen[i][0] + \")\") in em_prob:\n",
    "                    state_score *= em_prob[\"(\" + tags[j] + \", \" + sen[i][0] + \")\"]\n",
    "                else:\n",
    "                    state_score *= em_prob[\"(\" + tags[j] + \", \" + \"<unk>\" + \")\"]\n",
    "                \n",
    "                if(state_score > best_score):\n",
    "                    best_score = state_score\n",
    "                    highest_prob_tag = tags[j]\n",
    "                    \n",
    "            prev_tag = highest_prob_tag\n",
    "            seq.append(prev_tag)\n",
    "            score.append(best_score)\n",
    "        sequences.append(seq)\n",
    "        total_score.append(score)\n",
    "\n",
    "    return sequences, total_score\n",
    "\n",
    "sequences, total_score = greedy_decoding(trans_prob, em_prob, prior_prob, valid_sentences, tags)\n",
    "\n",
    "#measure_acc function computes the accuracy of our model by comparing the the predicted tag sequence \n",
    "#with groundtruth tag sequence\n",
    "def measure_acc(sequences, valid_sentences):\n",
    "    count = 0\n",
    "    corr_tag_count = 0\n",
    "    for i in range(len(valid_sentences)):\n",
    "        for j in range(len(valid_sentences[i])):\n",
    "\n",
    "            if(sequences[i][j] == valid_sentences[i][j][1]):\n",
    "                corr_tag_count += 1\n",
    "            count +=1\n",
    "    \n",
    "    acc = corr_tag_count / count\n",
    "    return acc\n",
    "\n",
    "print('Accuracy of our Greedy Decoding HMM model on Development data: {}'.format(measure_acc(sequences, valid_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below we read the test file, store the test data in a list( called sentences) consisting of lists(called sentence), where each sublist consists of tuples, where each tuple corresponds to one data row in test data and then use the greedy_decoding function to get the predicted tags for our test data. Furthermore we use the output_file function to store the predicted tags and words of the test data ,in the required format, in 'greedy.out' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the test file\n",
    "test_data = pd.read_csv(\"./data/test\", sep = '\\t', names = ['id', 'words'])\n",
    "test_data['occ'] = test_data.groupby('words')['words'].transform('size')\n",
    "test_data['words'] = test_data.apply(lambda row : replace(row), axis = 1)\n",
    "\n",
    "#storing the test data in a list( called sentences) consisting of lists(called sentence), \n",
    "#where each sublist consists of tuples, where each tuple corresponds to one data row in test data\n",
    "test_sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for row in test_data.itertuples():\n",
    "    if(row.id == 1 and first == 0):\n",
    "        test_sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append(row.words)\n",
    "test_sentences.append(sentence)\n",
    "\n",
    "test_sequences, test_score = greedy_decoding(trans_prob, em_prob, prior_prob, test_sentences, tags)\n",
    "\n",
    "#output_file function stores the predicted tag sequence of our HMM model on test dataset in an output file, \n",
    "#in the required format\n",
    "def output_file(test_inputs, test_outputs, filename):\n",
    "    res = []\n",
    "    for i in range(len(test_inputs)):\n",
    "        s = []\n",
    "        for j in range(len(test_inputs[i])):\n",
    "            s.append((str(j+1), test_inputs[i][j], test_outputs[i][j]))\n",
    "        res.append(s)\n",
    "    \n",
    "    with open(filename + \".out\", 'w') as f:\n",
    "        for ele in res:\n",
    "            f.write(\"\\n\".join([str(item[0]) + \"\\t\" + item[1] + \"\\t\" + item[2] for item in ele]))\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "#we use the output_file function to store the predictions of greedy decoding by our HMM model \n",
    "#on the test data in 'greedy.out'\n",
    "output_file(test_sentences, test_sequences, \"greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fourth task is to implement the viterbi decoding algorithm with HMM.\n",
    "## Task. Implementing the viterbi decoding algorithm and evaluate it on the development data. What is the accuracy on the dev data? Predicting the part-of-speech tags of the sentences in the test data and output the predictions in a file named viterbi.out, in the same format of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below we define the viterbi_decoding function that computes the probabilty for each word in a sentence having a tag from the group of all tags based on the dynamic programming algorithm called viterbi decoding. Then we use the function to calculate these probabilities for all sentences in the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi_decoding function computes the probabilty for each word in a sentence having a tag from the group of all tags\n",
    "#based on the dynamic programming algorithm called viterbi decoding \n",
    "def viterbi_decoding(trans_prob, em_prob, prior_prob, sen, tags):\n",
    "\n",
    "    n = len(tags)\n",
    "    viterbi_list = []\n",
    "    cache = {}\n",
    "    for si in tags:\n",
    "        if str(\"(\" + si + \", \" + sen[0][0] + \")\") in em_prob:\n",
    "            viterbi_list.append(prior_prob[si] * em_prob[\"(\" + si + \", \" + sen[0][0] + \")\"])\n",
    "        else:\n",
    "            #viterbi_list.append(1)\n",
    "            viterbi_list.append(prior_prob[si] * em_prob[\"(\" + si + \", \" + \"<unk>\" + \")\"])\n",
    "\n",
    "    for i, l in enumerate(sen):\n",
    "        word = l[0]\n",
    "        if i == 0: continue\n",
    "        temp_list = [None] * n\n",
    "        for j,tag in enumerate(tags):\n",
    "            score = -1\n",
    "            val = 1\n",
    "            for k, prob in enumerate(viterbi_list):\n",
    "                if str(\"(\" + tags[k] + \",\" + tag + \")\") in trans_prob and str(\"(\" + tag + \", \" + word + \")\") in em_prob:\n",
    "                    val = prob * trans_prob[\"(\" + tags[k] + \",\" + tag + \")\"] * em_prob[\"(\" + tag + \", \" + word + \")\"]\n",
    "                else:\n",
    "                   # val = 1\n",
    "                   val = prob * trans_prob[\"(\" + tags[k] + \",\" + tag + \")\"] * em_prob[\"(\" + tag + \", \" + \"<unk>\" + \")\"]\n",
    "                if(score < val):\n",
    "                    score = val\n",
    "                    cache[str(i) + \", \" + tag] = [tags[k], val]\n",
    "            temp_list[j] = score\n",
    "        viterbi_list = [x for x in temp_list]\n",
    "    \n",
    "    return cache, viterbi_list\n",
    "\n",
    "\n",
    "c = []\n",
    "v = []\n",
    "#we apply viterbi decoding function to all sentences in Validation data\n",
    "for sen in valid_sentences:\n",
    "    a, b = viterbi_decoding(trans_prob, em_prob, prior_prob, sen, tags)\n",
    "    c.append(a)\n",
    "    v.append(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below we define the function viterbi_backward that finds the best possible tag sequence for each sentence based on the probabilites calculated by the viterbi decoding function, we then apply the viterbi backward function to all sentences in Validation data and print the accuracy of our viterbi decoding HMM model using measure_acc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our Viterbi Decoding HMM model on Development data: 0.9480905834496994\n"
     ]
    }
   ],
   "source": [
    "#viterbi_backward function than finds the best possible tag sequence for each sentence based on the \n",
    "#probabilites calculated by the viterbi decoding function\n",
    "def viterbi_backward(tags, cache, viterbi_list):\n",
    "\n",
    "    num_states = len(tags)\n",
    "    n = len(cache) // num_states\n",
    "    best_sequence = []\n",
    "    best_sequence_breakdown = []\n",
    "    x = tags[np.argmax(np.asarray(viterbi_list))]\n",
    "    best_sequence.append(x)\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        val = cache[str(i) + ', ' + x][1]\n",
    "        x = cache[str(i) + ', ' + x][0]\n",
    "        best_sequence = [x] + best_sequence\n",
    "        best_sequence_breakdown =  [val] + best_sequence_breakdown\n",
    "    \n",
    "    return best_sequence, best_sequence_breakdown\n",
    "\n",
    "#we apply viterbi backward function to all sentences in Validation data\n",
    "best_seq = []\n",
    "best_seq_score = []\n",
    "for cache, viterbi_list in zip(c, v):\n",
    "\n",
    "    a, b = viterbi_backward(tags, cache, viterbi_list)\n",
    "    best_seq.append(a)\n",
    "    best_seq_score.append(b)\n",
    "\n",
    "print('Accuracy of our Viterbi Decoding HMM model on Development data: {}'.format(measure_acc(best_seq, valid_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: In the cell below we apply the viterbi_decoding function followed by the viterbi_backward function for all sentences in the test data. Then, we use the output_file function to store the predicted tags and words of the test data ,in the required format, in 'viterbi.out' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the viterbi decoding algorithm on all sentences in the test data using the functions viterbi_decoding\n",
    "#and viterbi_backward\n",
    "c = []\n",
    "v = []\n",
    "\n",
    "for sen in test_sentences:\n",
    "    a, b = viterbi_decoding(trans_prob, em_prob, prior_prob, sen, tags)\n",
    "    c.append(a)\n",
    "    v.append(b)\n",
    "\n",
    "best_seq = []\n",
    "best_seq_score = []\n",
    "for cache, viterbi_list in zip(c, v):\n",
    "\n",
    "    a, b = viterbi_backward(tags, cache, viterbi_list)\n",
    "    best_seq.append(a)\n",
    "    best_seq_score.append(b)\n",
    "\n",
    "#we use the output_file function to store the predictions of Viterbi decoding by our HMM model \n",
    "#on the test data in 'viterbi.out'\n",
    "output_file(test_sentences, best_seq, 'viterbi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05a30917fcfa4e776f7a314fdfe61b14f33821188b170261aa8594464f55dc4a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
